{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import re\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2',\n",
       " 'K',\n",
       " 'o',\n",
       " 'N',\n",
       " 'u',\n",
       " '9',\n",
       " 'Z',\n",
       " 'J',\n",
       " 'B',\n",
       " 'x',\n",
       " 'E',\n",
       " 'à',\n",
       " 'j',\n",
       " 'Y',\n",
       " 'U',\n",
       " 'W',\n",
       " 'M',\n",
       " ';',\n",
       " '“',\n",
       " 'è',\n",
       " 'T',\n",
       " 'c',\n",
       " 'æ',\n",
       " '—',\n",
       " 'X',\n",
       " 'f',\n",
       " '3',\n",
       " 'L',\n",
       " 'Q',\n",
       " 'q',\n",
       " 'O',\n",
       " '£',\n",
       " 'p',\n",
       " 'P',\n",
       " '0',\n",
       " 'd',\n",
       " ')',\n",
       " '!',\n",
       " '5',\n",
       " '\\n',\n",
       " 'v',\n",
       " 'é',\n",
       " 'â',\n",
       " '.',\n",
       " '-',\n",
       " '_',\n",
       " ',',\n",
       " 'œ',\n",
       " 'A',\n",
       " 'n',\n",
       " '6',\n",
       " 'I',\n",
       " 'G',\n",
       " '7',\n",
       " '(',\n",
       " 'w',\n",
       " 'i',\n",
       " 'V',\n",
       " 'l',\n",
       " 'R',\n",
       " 'g',\n",
       " 'm',\n",
       " 'b',\n",
       " 'h',\n",
       " 'D',\n",
       " 'a',\n",
       " '4',\n",
       " 'r',\n",
       " 'F',\n",
       " '”',\n",
       " '½',\n",
       " 'z',\n",
       " '’',\n",
       " ' ',\n",
       " '8',\n",
       " '‘',\n",
       " ':',\n",
       " 'S',\n",
       " 'C',\n",
       " 'k',\n",
       " '1',\n",
       " '&',\n",
       " 's',\n",
       " 'H',\n",
       " 'e',\n",
       " 't',\n",
       " 'y',\n",
       " '?']"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(\"./sherlock_holmes.txt\", \"r\")\n",
    "text = file.read()\n",
    "\n",
    "# consider text between start and end of project gutenberg. starts after 2nd \"***\", ends before 3rd \"***\" and we should not include \"***\"\n",
    "start = text.find(\"***\", text.find(\"***\") + 1)\n",
    "end = text.find(\"***\", start + 1)\n",
    "text = text[start+3:end]\n",
    "\n",
    "# remove all occurences of '&c'\n",
    "text = re.sub('&c', '', text)\n",
    "\n",
    "# replace new lines with single new line\n",
    "text=re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)\n",
    "text = re.sub(r'\\n+', '\\n', text)\n",
    "\n",
    "# replace multiple spaces with single space\n",
    "text = re.sub(' +', ' ', text)\n",
    "\n",
    "# find unique characters in the text\n",
    "list(set(text)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the data to format as tokens\n",
    "\n",
    "add space before and after\n",
    "- '-': hyphen used to connect words\n",
    "- '—': em dash to create interruptions in dialogue\n",
    "\n",
    "add space before\n",
    "- '½': this is used to represent half once in the text\n",
    "- '’': this is used to enclose quotations within the main dialogue (lets see if the model can learn this)\n",
    "- '?': questions\n",
    "- '!': exclamations\n",
    "- '.': this is used to end a sentence\n",
    "- ',': \n",
    "- ')': this is used to enclose additional information\n",
    "- '”': end quote\n",
    "- ';'\n",
    "\n",
    "add space after\n",
    "- '‘': this is used to enclose quotations within the main dialogue (lets see if the model can learn this)\n",
    "- '“': start quote\n",
    "- '(': this is used to enclose additional information\n",
    "\n",
    "as it is\n",
    "- '£': this is used to represent the British pound\n",
    "- '&': this is used to represent 'and' in company names\n",
    "\n",
    "for '_', '0123456789', '.', ':', add space before and after if there already isn't\n",
    "\n",
    "make sure the same for the \"\\n\" substring\n",
    "\n",
    "parts of a word:\n",
    "- 'æ': part of the word encyclopædia\n",
    "- 'à': letter used in French, Italian, and Portuguese. It is an 'a' with a grave accent\n",
    "- 'é': letter used in French, Spanish, and Portuguese. It is an 'e' with an acute accent\n",
    "- 'è': letter used in French, Italian, and Portuguese. It is an 'e' with a grave accent\n",
    "- 'â': letter used in French, Portuguese, and Vietnamese. It is an 'a' with a circumflex accent\n",
    "- 'a-z': this is used to represent letters\n",
    "- 'A-Z': this is used to represent letters\n",
    "\n",
    "we'll use ' ' to seperate words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to space-seperate per our vocabulary\n",
    "def tokenize(text):\n",
    "    formatted_text = \"\"\n",
    "\n",
    "    for i in range(len(text)):\n",
    "        char = text[i]\n",
    "        next_char = text[i + 1] if i < len(text) - 1 else None\n",
    "\n",
    "        if char in \"-—\":\n",
    "            formatted_text += \" \" + char + \" \"\n",
    "        elif char in \"½’?!,)”;\":\n",
    "            formatted_text += \" \" + char\n",
    "        elif char in \"‘“(\":\n",
    "            formatted_text += char + \" \"\n",
    "        elif char in \"_0123456789.:\":\n",
    "            if next_char and (not next_char==\" \") and (not text[i - 1]==\" \"):\n",
    "                formatted_text += \" \" + char + \" \"\n",
    "            elif next_char and next_char==\" \" and (not text[i - 1]==\" \"):\n",
    "                formatted_text += \" \" + char\n",
    "            elif next_char and not next_char==\" \":\n",
    "                formatted_text += char + \" \"\n",
    "            else:\n",
    "                formatted_text += char\n",
    "        else:\n",
    "            formatted_text += char\n",
    "\n",
    "    result = \"\"\n",
    "    for i in range(len(formatted_text)):\n",
    "        if formatted_text[i] == \"\\n\":\n",
    "            # Check if there's a character before '\\n' and if it's not a space\n",
    "            if i > 0 and formatted_text[i - 1] != \" \":\n",
    "                result += \" \"\n",
    "            \n",
    "            # Add the newline character itself\n",
    "            result += \"\\n\"\n",
    "            \n",
    "            # Check if there's a character after '\\n' and if it's not a space\n",
    "            if i < len(formatted_text) - 1 and formatted_text[i + 1] != \" \":\n",
    "                result += \" \"\n",
    "        else:\n",
    "            result += formatted_text[i]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Vocabulary and Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = sorted(list(set(re.split(r'[ \\t\\r\\f\\v]+', text))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8459\n"
     ]
    }
   ],
   "source": [
    "wtoi = {ch:i for i, ch in enumerate(vocabulary)}\n",
    "itow = {i:ch for i, ch in enumerate(vocabulary)}\n",
    "\n",
    "# add '#' to the wtoi and itow mappings to represent empty\n",
    "index = len(wtoi)\n",
    "wtoi['#'] = index\n",
    "itow[index] = '#'\n",
    "\n",
    "print(len(itow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_functions = {\n",
    "    \"tanh\": torch.tanh,\n",
    "    \"relu\": torch.relu\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextToken(nn.Module):\n",
    "  def __init__(self, block_size, vocab_size, emb_dim, hidden_size_1, hidden_size_2, activation):\n",
    "    # Initialize the parent class (nn.Module)\n",
    "    super().__init__()\n",
    "\n",
    "    # Create an embedding layer to map each character to a vector\n",
    "    self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "\n",
    "    # hidden layers\n",
    "    self.lin1 = nn.Linear(block_size * emb_dim, hidden_size_1)\n",
    "    self.lin2 = nn.Linear(hidden_size_1, hidden_size_2)\n",
    "\n",
    "    # Create a linear layer to map the hidden state to the vocabulary size\n",
    "    self.lin3 = nn.Linear(hidden_size_2, vocab_size)\n",
    "\n",
    "    self.activation = activation\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Embed the input characters\n",
    "    x = self.emb(x)\n",
    "\n",
    "    # Reshape the embedding to a 2D tensor\n",
    "    # Before the dimension was (batch_size, block_size, emb_dim) and now its (batch_size, block_size * emb_dim)\n",
    "    x = x.view(x.shape[0], -1)\n",
    "\n",
    "    x = self.activation(self.lin1(x))\n",
    "    x = self.activation(self.lin2(x))\n",
    "\n",
    "    x = self.lin3(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model_5_64_tanh\n",
      "0 17.1767578125\n",
      "100 1.5295231342315674\n",
      "200 1.2396035194396973\n",
      "300 1.3238580226898193\n",
      "400 1.373178243637085\n",
      "500 1.2566912174224854\n",
      "600 1.3559234142303467\n",
      "700 1.4627068042755127\n",
      "==\n",
      "Model saved as next_word_model_5_64_tanh.pt\n",
      "=====================\n",
      "training model_5_64_relu\n",
      "0 5.917835712432861\n",
      "100 0.10576169192790985\n",
      "200 0.11086101830005646\n",
      "300 0.5188949108123779\n",
      "400 0.30586305260658264\n",
      "500 0.27896881103515625\n",
      "600 0.12550616264343262\n",
      "700 0.22407464683055878\n",
      "==\n",
      "Model saved as next_word_model_5_64_relu.pt\n",
      "=====================\n",
      "training model_5_128_tanh\n",
      "0 14.542134284973145\n",
      "100 1.0339953899383545\n",
      "200 1.3775618076324463\n",
      "300 1.201419711112976\n",
      "400 1.4258949756622314\n",
      "500 1.4717528820037842\n",
      "600 1.4162380695343018\n",
      "700 1.4264135360717773\n",
      "==\n",
      "Model saved as next_word_model_5_128_tanh.pt\n",
      "=====================\n",
      "training model_5_128_relu\n",
      "0 5.782036304473877\n",
      "100 0.10380765795707703\n",
      "200 0.40831178426742554\n",
      "300 0.40738949179649353\n",
      "400 0.3708231747150421\n",
      "500 0.27905502915382385\n",
      "600 0.39998507499694824\n",
      "700 0.3796998858451843\n",
      "==\n",
      "Model saved as next_word_model_5_128_relu.pt\n",
      "=====================\n",
      "training model_10_64_tanh\n",
      "0 57.71962356567383\n",
      "100 17.29853630065918\n",
      "200 1.2146656513214111\n",
      "300 0.39209386706352234\n",
      "400 1.2316242456436157\n",
      "500 0.8766473531723022\n",
      "600 0.730747640132904\n",
      "700 0.6737368702888489\n",
      "==\n",
      "Model saved as next_word_model_10_64_tanh.pt\n",
      "=====================\n",
      "training model_10_64_relu\n",
      "0 6.058709144592285\n",
      "100 0.0016336515545845032\n",
      "200 0.0013298639096319675\n",
      "300 0.21583551168441772\n",
      "400 0.244111567735672\n",
      "500 0.18148931860923767\n",
      "600 0.1496679037809372\n",
      "700 0.20310698449611664\n",
      "==\n",
      "Model saved as next_word_model_10_64_relu.pt\n",
      "=====================\n",
      "training model_10_128_tanh\n",
      "0 63.61756134033203\n",
      "100 15.789461135864258\n",
      "200 1.0709900856018066\n",
      "300 0.6868889927864075\n",
      "400 0.8223374485969543\n",
      "500 0.6939628720283508\n",
      "600 0.8012319803237915\n",
      "700 0.7924439907073975\n",
      "==\n",
      "Model saved as next_word_model_10_128_tanh.pt\n",
      "=====================\n",
      "training model_10_128_relu\n",
      "0 6.192435264587402\n",
      "100 2.016408920288086\n",
      "200 1.1458022594451904\n",
      "300 0.8033765554428101\n",
      "400 0.7768628001213074\n",
      "500 0.6960203647613525\n",
      "600 0.44432154297828674\n",
      "700 0.6928240656852722\n",
      "==\n",
      "Model saved as next_word_model_10_128_relu.pt\n",
      "=====================\n",
      "training model_15_64_tanh\n",
      "0 64.73825073242188\n",
      "100 15.5647554397583\n",
      "200 3.465670347213745\n",
      "300 0.6894968152046204\n",
      "400 1.1599018573760986\n",
      "500 0.6268890500068665\n",
      "600 0.5948762893676758\n",
      "700 0.7910194396972656\n",
      "==\n",
      "Model saved as next_word_model_15_64_tanh.pt\n",
      "=====================\n",
      "training model_15_64_relu\n",
      "0 6.26317024230957\n",
      "100 2.709709644317627\n",
      "200 1.1473480463027954\n",
      "300 0.9840860366821289\n",
      "400 0.5199204683303833\n",
      "500 0.5389942526817322\n",
      "600 1.0870122909545898\n",
      "700 0.31976041197776794\n",
      "==\n",
      "Model saved as next_word_model_15_64_relu.pt\n",
      "=====================\n",
      "training model_15_128_tanh\n",
      "0 38.54172897338867\n",
      "100 12.416947364807129\n",
      "200 0.27844345569610596\n",
      "300 0.7559667229652405\n",
      "400 0.6110314726829529\n",
      "500 0.8638446927070618\n",
      "600 0.6915326118469238\n",
      "700 0.72674560546875\n",
      "==\n",
      "Model saved as next_word_model_15_128_tanh.pt\n",
      "=====================\n",
      "training model_15_128_relu\n",
      "0 6.105169773101807\n",
      "100 0.28049972653388977\n",
      "200 0.49383872747421265\n",
      "300 0.40714967250823975\n",
      "400 0.33142024278640747\n",
      "500 0.37999945878982544\n",
      "600 0.2624189257621765\n",
      "700 0.19331517815589905\n",
      "==\n",
      "Model saved as next_word_model_15_128_relu.pt\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "for block_size in [5, 10, 15]:\n",
    "    \n",
    "    # generating training data\n",
    "    X, Y = [], []\n",
    "    context = [index] * block_size\n",
    "    for word in [item for item in text.split(\" \") if item!=\"\"]:\n",
    "        ix = wtoi[word]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        context = context[1:] + [ix]\n",
    "    X = torch.tensor(X).to(device)\n",
    "    Y = torch.tensor(Y).to(device)\n",
    "    \n",
    "    for embedding_dimensions in [64, 128]:\n",
    "        for activation in [\"tanh\", \"relu\"]:\n",
    "            \n",
    "            model = NextToken(block_size, len(wtoi), embedding_dimensions, 2048, 2048, activation_functions[activation]).to(device)\n",
    "            \n",
    "            # Train the model\n",
    "            print(f\"training model_{block_size}_{embedding_dimensions}_{activation}\")\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            opt = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "            # Mini-batch training\n",
    "            batch_size = 4096\n",
    "            print_every = 100\n",
    "            for epoch in range(750):\n",
    "                for i in range(0, X.shape[0], batch_size):\n",
    "                    x = X[i:i+batch_size]\n",
    "                    y = Y[i:i+batch_size] \n",
    "                    y_pred = model(x) \n",
    "                    loss = loss_fn(y_pred, y) \n",
    "                    loss.backward()\n",
    "                    opt.step()\n",
    "                    opt.zero_grad()\n",
    "                if epoch % print_every == 0:\n",
    "                    print(epoch, loss.item())\n",
    "            print(\"==\")\n",
    "\n",
    "            # Saving the model\n",
    "            filename = f\"question_1_models/next_word_model_{block_size}_{embedding_dimensions}_{activation}.pt\"\n",
    "            torch.save(model.state_dict(), filename)\n",
    "            print(f\"Model saved as {filename[len(\"question_1_models/\"):]}\")\n",
    "            print(\"=====================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_es335_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
